{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import svds, eigs\n",
    "import utils,models,time,sys\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import eval_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_users,\n",
    "                 n_items,\n",
    "                 n_factors=40,\n",
    "                 dropout_p=0,\n",
    "                 sparse=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_users : int\n",
    "            Number of users\n",
    "        n_items : int\n",
    "            Number of items\n",
    "        n_factors : int\n",
    "            Number of latent factors (or embeddings or whatever you want to\n",
    "            call it).\n",
    "        dropout_p : float\n",
    "            p in nn.Dropout module. Probability of dropout.\n",
    "        sparse : bool\n",
    "            Whether or not to treat embeddings as sparse. NOTE: cannot use\n",
    "            weight decay on the optimizer if sparse=True. Also, can only use\n",
    "            Adagrad.\n",
    "        \"\"\"\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "        self.user_biases = torch.nn.Embedding(n_users, 1, sparse=sparse)\n",
    "        self.item_biases = torch.nn.Embedding(n_items, 1, sparse=sparse)\n",
    "        self.user_embeddings = torch.nn.Embedding(n_users, n_factors, sparse=sparse)\n",
    "        self.item_embeddings = torch.nn.Embedding(n_items, n_factors, sparse=sparse)\n",
    "        \n",
    "        self.dropout_p = dropout_p\n",
    "        self.dropout = torch.nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "        self.sparse = sparse\n",
    "        \n",
    "    def forward(self, users, items):\n",
    "        \"\"\"\n",
    "        Forward pass through the model. For a single user and item, this\n",
    "        looks like:\n",
    "        user_bias + item_bias + user_embeddings.dot(item_embeddings)\n",
    "        Parameters\n",
    "        ----------\n",
    "        users : np.ndarray\n",
    "            Array of user indices\n",
    "        items : np.ndarray\n",
    "            Array of item indices\n",
    "        Returns\n",
    "        -------\n",
    "        preds : np.ndarray\n",
    "            Predicted ratings.\n",
    "        \"\"\"\n",
    "        ues = self.user_embeddings(users)\n",
    "        uis = self.item_embeddings(items)\n",
    "\n",
    "        #preds = self.user_biases(users)\n",
    "        #preds += self.item_biases(items)\n",
    "        preds2 = (self.dropout(ues) * self.dropout(uis)).sum(1)\n",
    "        return preds2\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def predict(self, users, items):\n",
    "        return self.forward(users, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training and testing file.\n"
     ]
    }
   ],
   "source": [
    "dataset_name='msd-exp-40-160'\n",
    "train_dir = \"../train/\"\n",
    "print 'reading training and testing file.'\n",
    "\n",
    "n_users, n_items, train_data = utils.load_train_data(os.path.join(train_dir, '%s.train.rating' % dataset_name))\n",
    "testRatings =  utils.load_rating_file_as_list(\"../train/%s.test.rating\" % dataset_name)\n",
    "testNegatives = utils.load_negative_file(\"../train/%s.test.negative\" % dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[148, 299, 476, 688, 1617, 1664]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].nonzero()[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e27ea7b31b5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "hit = 100\n",
    "a = [1,10,9]\n",
    "a.pop\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_tuple(train_data, users, n_items, neg_num=10):\n",
    "    u_list = []\n",
    "    i_list = []\n",
    "    r_list = []\n",
    "    for u in users:\n",
    "        neg_sample = np.random.choice(n_items, 100)\n",
    "        pos_items = train_data[u].nonzero()[1].tolist()\n",
    "        for pos_item in pos_items:\n",
    "            neg_sample = np.random.choice(n_items, 10).tolist()\n",
    "            if pos_item in neg_sample:\n",
    "                neg_sample.remove(pos_item)\n",
    "            u_list += [u]*(len(neg_sample)+1)\n",
    "            i_list.append(pos_item)\n",
    "            r_list.append(1.0)\n",
    "            i_list += neg_sample\n",
    "            r_list += [0.0] * len(neg_sample)\n",
    "    return u_list, i_list, r_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, test_size = 10000, HR = 0.0962, NDCG = 0.0430, loss = 15507809.0000\n",
      "epoch = 1, test_size = 10000, HR = 0.0952, NDCG = 0.0426, loss = 10190454.0000\n",
      "epoch = 2, test_size = 10000, HR = 0.0954, NDCG = 0.0426, loss = 6650565.5000\n",
      "epoch = 3, test_size = 10000, HR = 0.0957, NDCG = 0.0430, loss = 4336228.5000\n",
      "epoch = 4, test_size = 10000, HR = 0.0945, NDCG = 0.0426, loss = 2857521.2500\n",
      "epoch = 5, test_size = 10000, HR = 0.0945, NDCG = 0.0428, loss = 1900734.3750\n",
      "epoch = 6, test_size = 10000, HR = 0.0926, NDCG = 0.0421, loss = 1268879.1250\n",
      "epoch = 7, test_size = 10000, HR = 0.0931, NDCG = 0.0423, loss = 859844.5000\n",
      "epoch = 8, test_size = 10000, HR = 0.0936, NDCG = 0.0425, loss = 590780.3750\n",
      "epoch = 9, test_size = 10000, HR = 0.0931, NDCG = 0.0425, loss = 417914.8125\n",
      "epoch = 10, test_size = 10000, HR = 0.0963, NDCG = 0.0434, loss = 303211.9688\n",
      "epoch = 11, test_size = 10000, HR = 0.0986, NDCG = 0.0445, loss = 231273.1875\n",
      "epoch = 12, test_size = 10000, HR = 0.0985, NDCG = 0.0446, loss = 183960.5625\n",
      "epoch = 13, test_size = 10000, HR = 0.1027, NDCG = 0.0456, loss = 153701.0469\n",
      "epoch = 14, test_size = 10000, HR = 0.1068, NDCG = 0.0474, loss = 134689.3594\n",
      "epoch = 15, test_size = 10000, HR = 0.1155, NDCG = 0.0510, loss = 122866.3828\n",
      "epoch = 16, test_size = 10000, HR = 0.1216, NDCG = 0.0542, loss = 115599.5156\n",
      "epoch = 17, test_size = 10000, HR = 0.1262, NDCG = 0.0575, loss = 110876.9766\n",
      "epoch = 18, test_size = 10000, HR = 0.1329, NDCG = 0.0620, loss = 108139.2266\n",
      "epoch = 19, test_size = 10000, HR = 0.1362, NDCG = 0.0662, loss = 106382.7031\n",
      "epoch = 20, test_size = 10000, HR = 0.1388, NDCG = 0.0698, loss = 105351.1094\n",
      "epoch = 21, test_size = 10000, HR = 0.1445, NDCG = 0.0734, loss = 104753.8594\n",
      "epoch = 22, test_size = 10000, HR = 0.1431, NDCG = 0.0733, loss = 104399.4297\n",
      "epoch = 23, test_size = 10000, HR = 0.1411, NDCG = 0.0735, loss = 104216.9219\n",
      "epoch = 24, test_size = 10000, HR = 0.1444, NDCG = 0.0749, loss = 104140.1484\n",
      "epoch = 25, test_size = 10000, HR = 0.1439, NDCG = 0.0761, loss = 104118.0859\n",
      "epoch = 26, test_size = 10000, HR = 0.1464, NDCG = 0.0756, loss = 104126.6094\n",
      "epoch = 27, test_size = 10000, HR = 0.1454, NDCG = 0.0769, loss = 104148.2969\n",
      "epoch = 28, test_size = 10000, HR = 0.1474, NDCG = 0.0781, loss = 104191.8906\n",
      "epoch = 29, test_size = 10000, HR = 0.1488, NDCG = 0.0776, loss = 104244.6328\n",
      "epoch = 30, test_size = 10000, HR = 0.1447, NDCG = 0.0769, loss = 104294.8203\n",
      "epoch = 31, test_size = 10000, HR = 0.1476, NDCG = 0.0781, loss = 104365.3984\n",
      "epoch = 32, test_size = 10000, HR = 0.1460, NDCG = 0.0780, loss = 104420.2969\n",
      "epoch = 33, test_size = 10000, HR = 0.1479, NDCG = 0.0775, loss = 104483.1172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-3b442d5b3417>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmf_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestRatings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestNegatives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mhr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndcgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch = %d, test_size = %d, HR = %.4f, NDCG = %.4f, loss = %.4f'\u001b[0m  \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shenfan/Code/Project/AlgoComp/MSD_Recommendation/src/eval_utils.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, num_thread, limit_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Single thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mndcg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_one_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mhits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shenfan/Code/Project/AlgoComp/MSD_Recommendation/src/eval_utils.py\u001b[0m in \u001b[0;36meval_one_rating\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morigin_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mmap_item_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morigin_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@2/2.7.14_3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/heapq.pyc\u001b[0m in \u001b[0;36mnlargest\u001b[0;34m(n, iterable, key)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0min1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mizip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min2\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# decorate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m                       \u001b[0;31m# undecorate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(n_users, n_items, n_factors=16)\n",
    "loss_fn = torch.nn.MSELoss(size_average=False) \n",
    "#loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=5e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "N = train_data.shape[0]\n",
    "idxlist = range(N)\n",
    "batch_size = 256\n",
    "for epoch in range(num_epochs):\n",
    "    np.random.shuffle(idxlist)\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for bnum, st_idx in enumerate(range(0, N, batch_size)):\n",
    "            end_idx = min(st_idx + batch_size, N)\n",
    "            u,i,r = make_training_tuple(train_data, idxlist[st_idx:end_idx], n_items, 20)\n",
    "            #X = train_data[idxlist[st_idx:end_idx]].tocoo()\n",
    "            users = Variable(torch.LongTensor(u))\n",
    "            items = Variable(torch.LongTensor(i))\n",
    "            ratings = Variable(torch.FloatTensor(r))\n",
    "            prediction = model(users, items)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(prediction, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    topk = 10\n",
    "    limit = 10000\n",
    "    evaluation = eval_utils.mf_evaluation(model, testRatings, testNegatives, topk)\n",
    "    (hits, ndcgs) = evaluation.evaluate(1, limit_size=limit)\n",
    "    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print('epoch = %d, test_size = %d, HR = %.4f, NDCG = %.4f, loss = %.4f'  % (epoch, len(hits), hr, ndcg, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sparse.csc_matrix([[1, 0, 0], [5, 0, 2], [0, -1, 0], [0, 0, 3]], dtype=float)\n",
    "u, s, vt = svds(A, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.00000000e+00, -6.43251766e-18,  2.00000000e+00])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(u[1],(s*vt.T).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vt = svds(train_data, k=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.matmul(u,(s*vt.T).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_size = 44967, HR = 0.3433, NDCG = 0.1959\n"
     ]
    }
   ],
   "source": [
    "evaluator = eval_utils.vaecf_evaluation(pred, testRatings, testNegatives, topk)\n",
    "(hits, ndcgs) = evaluator.evaluate(1, limit_size=100000)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print('test_size = %d, HR = %.4f, NDCG = %.4f'  % ( len(hits), hr, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonzeros(m, row):\n",
    "    \"\"\" returns the non zeroes of a row in csr_matrix \"\"\"\n",
    "    for index in range(m.indptr[row], m.indptr[row+1]):\n",
    "        yield m.indices[index], m.data[index]\n",
    "\n",
    "def alternating_least_squares(Cui, factors, regularization, iterations=20):\n",
    "    users, items = Cui.shape\n",
    "\n",
    "    X = np.random.rand(users, factors) * 0.01\n",
    "    Y = np.random.rand(items, factors) * 0.01\n",
    "\n",
    "    Ciu = Cui.T.tocsr()\n",
    "    for iteration in range(iterations):\n",
    "        print iteration\n",
    "        least_squares(Cui, X, Y, regularization)\n",
    "        least_squares(Ciu, Y, X, regularization)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def least_squares(Cui, X, Y, regularization):\n",
    "    users, factors = X.shape\n",
    "    YtY = Y.T.dot(Y)\n",
    "\n",
    "    for u in range(users):\n",
    "        # accumulate YtCuY + regularization * I in A\n",
    "        A = YtY + regularization * np.eye(factors)\n",
    "\n",
    "        # accumulate YtCuPu in b\n",
    "        b = np.zeros(factors)\n",
    "\n",
    "        for i, confidence in nonzeros(Cui, u):\n",
    "            factor = Y[i]\n",
    "            A += (confidence - 1) * np.outer(factor, factor)\n",
    "            b += confidence * factor\n",
    "\n",
    "        # Xu = (YtCuY + regularization * I)^-1 (YtCuPu)\n",
    "        X[u] = np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "X, Y = alternating_least_squares(train_data, 32, 0.1, iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.dot(X,Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS, test_size = 100000, HR = 0.4807, NDCG = 0.3022\n"
     ]
    }
   ],
   "source": [
    "topk=10\n",
    "evaluator = eval_utils.vaecf_evaluation(pred, testRatings, testNegatives, topk)\n",
    "(hits, ndcgs) = evaluator.evaluate(1, limit_size=100000)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print('ALS, test_size = %d, HR = %.4f, NDCG = %.4f'  % ( len(hits), hr, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_item_popularity(train_data, testRatings, testNegatives, topk=10, limit=100000):\n",
    "    item_pop = {}\n",
    "    for i in range(n_items):\n",
    "        item_pop[i] = train_data[:,i].count_nonzero()\n",
    "    item_pop_evaluation = eval_utils.item_popularity_evaluation(item_pop, testRatings, testNegatives, topk)\n",
    "    (hits, ndcgs) = item_pop_evaluation.evaluate(num_thread=1, limit_size=limit)\n",
    "    item_pop_hr, item_pop_ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "    print 'Item Popularity, testSize=%d, HitRate=%.4f, NDCG=%.4f' % (len(hits), item_pop_hr, item_pop_ndcg)\n",
    "    return item_pop_hr, item_pop_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Popularity, testSize=100000, HitRate=0.3690, NDCG=0.2194\n"
     ]
    }
   ],
   "source": [
    "hr1, ndcg1 = test_item_popularity(train_data, testRatings, testNegatives)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
