{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import sys,utils,models,time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading user-song matrix.\n",
      "Song: 386213 Listened Song: 163206\n",
      "User: 110000 Active User: 110000\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"../train/\"\n",
    "test_dir = \"../test/\"\n",
    "train_triplets_file=train_dir+\"kaggle_visible_evaluation_triplets.txt\"\n",
    "test_triplets_file=train_dir+\"year1_valid_triplets_hidden.txt\"\n",
    "user_file = train_dir+'kaggle_users.txt'\n",
    "song_file = train_dir+'kaggle_songs.txt'\n",
    "\n",
    "print \"loading user-song matrix.\"    \n",
    "user2id = {v:int(k) for k,v in enumerate(utils.load_users(user_file))}\n",
    "song2id = {k:int(v) for k,v in utils.song_to_idx(song_file).items()}\n",
    "\n",
    "s2u=utils.song_to_users(train_triplets_file,user2id,song2id)\n",
    "print \"Song:\",len(song2id),\"Listened Song:\",len(s2u)\n",
    "u2s=utils.user_to_songs(train_triplets_file,user2id,song2id)\n",
    "print \"User:\",len(user2id),\"Active User:\",len(u2s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file='../train/song_embedding_128.deepwalk'\n",
    "model = KeyedVectors.load_word2vec_format(embeddings_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([58821, 87433, 123630, 351764, 68212, 25150])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'123630', 0.8817476630210876),\n",
       " (u'12985', 0.8367016911506653),\n",
       " (u'25150', 0.8291943073272705),\n",
       " (u'351764', 0.8251831531524658),\n",
       " (u'68212', 0.8052948713302612),\n",
       " (u'307202', 0.7937827706336975),\n",
       " (u'288653', 0.7857335805892944),\n",
       " (u'311604', 0.7789132595062256),\n",
       " (u'87433', 0.7662076354026794),\n",
       " (u'92547', 0.7626552581787109)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = []\n",
    "print u2s[0]\n",
    "for s in u2s[0]:\n",
    "     embeds.append(model.get_vector(str(s)))\n",
    "user_vector = np.mean(embeds, axis=0)\n",
    "model.similar_by_vector(user_vector, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def negative_sample(user, cnt):\n",
    "    candidates = [s for s in s2u.keys() if s not in u2s[user]]\n",
    "    prob = np.array([len(s2u[x]) for x in candidates])\n",
    "    return np.random.choice(candidates,cnt,p=prob*1.0/sum(prob))\n",
    "\n",
    "def get_embeddings(users):\n",
    "    user_embeddings = []  # list\n",
    "    user_action_embs = [] # list\n",
    "    neg_embs = [] # list of list\n",
    "    neg_cnt = 5\n",
    "    for u in users:\n",
    "        s_embs = {}\n",
    "        for s in u2s[u]:\n",
    "            if str(s) in model.vocab:\n",
    "                s_embs[s] = model.get_vector(str(s))\n",
    "        for s in s_embs:\n",
    "            leave_one_out = [s_embs[si] for si in s_embs if si != s]\n",
    "            user_embeddings.append(np.mean(leave_one_out, axis=0))\n",
    "            user_action_embs.append(s_embs[s])\n",
    "            user_neg_samples = []\n",
    "            for neg in negative_sample(u, neg_cnt):\n",
    "                if str(neg) in model.vocab:\n",
    "                    user_neg_samples.append(model.get_vector(str(neg)))\n",
    "            neg_embs.append(user_neg_samples)\n",
    "    return user_embeddings, user_action_embs, neg_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings, user_action_embs, negative_samples = get_embeddings([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LTR(torch.nn.Module):\n",
    "    def __init__(self, D_in, H=32):\n",
    "        super(LTR, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return torch.sigmoid(y_pred)\n",
    "\n",
    "    def neg_loss(self, user_embedding, negative_embeddings):\n",
    "        scores = []\n",
    "        for ne in negative_embeddings:\n",
    "            score = self.forward(torch.tensor(np.multiply(user_embedding , ne)))\n",
    "            scores.append(score)\n",
    "        return torch.mean(torch.tensor(scores))\n",
    "\n",
    "    # users = list, user_actions = list, negatives = map\n",
    "    def loss(self, user_embeddings, user_action_embs, negatives):\n",
    "        loss = 0\n",
    "        for i, user_embedding in enumerate(user_embeddings):\n",
    "            pos_score = self.forward(torch.tensor(np.multiply(user_embedding,user_action_embs[i])))\n",
    "            neg_score = self.neg_loss(user_embedding, negatives[i])\n",
    "            loss = loss + neg_score-pos_score\n",
    "        return loss\n",
    "\n",
    "embedding_dim = 128\n",
    "LTRmodel = LTR(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(u2s)\n",
    "rand_indices = np.random.permutation(u2s.keys())\n",
    "split_pos = int(num_users * 0.7)\n",
    "\n",
    "train_users = rand_indices[:split_pos]\n",
    "valid_users = rand_indices[split_pos:]\n",
    "\n",
    "# model training\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD(LTRmodel.parameters(), lr=learning_rate)\n",
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 39.7040579319 0.19899481535\n",
      "1 34.4842088223 0.241581439972\n",
      "2 37.550951004 0.167332708836\n",
      "3 35.9972789288 0.415518283844\n",
      "4 32.2171998024 0.173104763031\n",
      "5 30.5230119228 -0.0439894795418\n",
      "6 34.064964056 0.236192405224\n",
      "7 33.5145881176 0.357805430889\n",
      "8 31.7265470028 0.229601562023\n",
      "9 30.2553269863 0.182775199413\n",
      "10 23.354706049 0.165178060532\n",
      "11 33.4721720219 0.447008311749\n",
      "12 28.3315420151 0.284649252892\n",
      "13 29.8890559673 0.221496403217\n",
      "14 21.7300209999 0.377642571926\n",
      "15 28.1625730991 0.246895551682\n",
      "16 28.7426891327 0.135021924973\n",
      "17 29.238106966 0.13764411211\n",
      "18 26.2274849415 0.321985602379\n",
      "19 29.1849589348 0.105818152428\n",
      "20 36.2828021049 0.50313013792\n",
      "21 23.138117075 0.0908598899841\n",
      "22 26.2779741287 0.176581203938\n",
      "23 22.0683288574 0.188812673092\n",
      "24 28.0011918545 0.196877062321\n",
      "25 36.2568790913 0.307262003422\n",
      "26 36.4432480335 0.138478457928\n",
      "27 29.0282199383 0.197724103928\n",
      "28 22.2350361347 0.101239562035\n",
      "29 32.3237440586 0.26782220602\n",
      "30 26.3298380375 0.223353266716\n",
      "31 31.4455490112 0.117894232273\n",
      "32 22.0179429054 0.103927195072\n",
      "33 23.5849339962 0.172538399696\n",
      "34 37.586974144 -0.134705781937\n",
      "35 20.5494849682 0.158325135708\n",
      "36 28.7048408985 0.064758181572\n",
      "37 28.3379809856 0.221491634846\n",
      "38 27.3832910061 0.127996325493\n",
      "39 33.9311339855 0.15036636591\n",
      "40 27.9849970341 0.238010883331\n",
      "41 26.6411659718 0.409713745117\n",
      "42 31.2341661453 0.284406840801\n",
      "43 40.6752159595 0.322964191437\n",
      "44 31.8628869057 0.087364256382\n",
      "45 27.4186909199 0.151554346085\n",
      "46 24.1670491695 0.191936910152\n",
      "47 25.596850872 0.203956127167\n",
      "48 28.734566927 0.36146146059\n",
      "49 41.2714810371 0.36720430851\n",
      "50 31.1531760693 0.348928868771\n",
      "51 20.7618060112 0.172855973244\n",
      "52 32.2382490635 0.362440884113\n",
      "53 20.4258198738 0.106089174747\n",
      "54 31.5580399036 0.289829611778\n",
      "55 35.809472084 0.405957460403\n",
      "56 35.1322901249 0.357761204243\n",
      "57 32.0948631763 0.154267132282\n",
      "58 35.803645134 0.240552246571\n",
      "59 35.1179521084 0.365741431713\n",
      "60 34.4134590626 0.195684015751\n",
      "61 35.8451969624 0.262095928192\n",
      "62 23.4593560696 0.356286644936\n",
      "63 37.1605789661 -0.00218695402145\n",
      "64 33.5599169731 0.318839609623\n",
      "65 35.0865399837 0.948226153851\n",
      "66 26.4874789715 0.224088609219\n",
      "67 32.2246451378 0.305281281471\n",
      "68 28.3729410172 0.369210600853\n",
      "69 33.416410923 0.204095959663\n",
      "70 40.6911230087 0.200137317181\n",
      "71 28.8512358665 0.269423961639\n",
      "72 32.8587319851 0.502809405327\n",
      "73 26.4791240692 0.136956810951\n",
      "74 28.8489189148 0.148764133453\n",
      "75 33.2174580097 0.304597616196\n",
      "76 29.2397549152 0.259994208813\n",
      "77 30.7339429855 0.0851423740387\n",
      "78 34.0067059994 0.573922812939\n",
      "79 29.6961219311 0.0768315792084\n",
      "80 35.3482069969 0.32023447752\n",
      "81 36.0545139313 0.191183924675\n",
      "82 21.1000370979 0.251774907112\n",
      "83 29.6246089935 0.0505159497261\n",
      "84 28.874243021 0.399641335011\n",
      "85 34.3601589203 0.0906407833099\n",
      "86 30.3779759407 0.173187434673\n",
      "87 34.9538609982 0.175516605377\n",
      "88 31.8959069252 0.390208661556\n",
      "89 23.8163878918 0.0371480584145\n",
      "90 40.5445039272 0.194493353367\n",
      "91 34.6624531746 0.334580123425\n",
      "92 33.8737270832 0.281708180904\n",
      "93 39.5187599659 0.288349330425\n",
      "94 29.6837530136 0.198291957378\n",
      "95 23.6922380924 0.118924617767\n",
      "96 29.7532310486 0.259471476078\n",
      "97 28.191177845 0.347116470337\n",
      "98 27.8295879364 0.189471304417\n",
      "99 25.4462049007 0.100718080997\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    batch_users = train_users[:16]\n",
    "    user_embeddings,user_action_embs,negatives = get_embeddings(batch_users) # TODO\n",
    "    loss = LTRmodel.loss(user_embeddings, user_action_embs, negatives)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    times.append(end_time-start_time)\n",
    "    print epoch, times[-1] ,loss.item()\n",
    "    random.shuffle(train_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0006198287010192871"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
